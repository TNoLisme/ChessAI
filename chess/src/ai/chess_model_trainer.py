import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from typing import Dict, Tuple, List, Optional

from chess_data_loader import ChessDataLoader
from chess_data_preprocessor import ChessDataPreprocessor
from chess_model import ChessModel

class ChessModelTrainer:
    """
    Class quáº£n lÃ½ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh AI cá» vua.
    """
    
    def __init__(self, data_folder: str = "data", model_save_dir: str = "models", history_length: int = 8):
        """
        Khá»Ÿi táº¡o trainer.

        Args:
            data_folder (str): ThÆ° má»¥c chá»©a dá»¯ liá»‡u .npy.
            model_save_dir (str): ThÆ° má»¥c lÆ°u mÃ´ hÃ¬nh.
            history_length (int): Äá»™ dÃ i lá»‹ch sá»­ nÆ°á»›c Ä‘i.
        """
        self.data_folder = data_folder
        self.model_save_dir = model_save_dir
        self.history_length = history_length
        # os.makedirs(model_save_dir, exist_ok=True)
        self.data_loader = ChessDataLoader(data_folder=data_folder)
        self.preprocessor = ChessDataPreprocessor(history_length=history_length)

    def prepare_datasets(self, batch_size: int = 32, val_split: float = 0.1, test_split: float = 0.1, 
                         shuffle: bool = True) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:
        """
        Chuáº©n bá»‹ cÃ¡c dataset cho huáº¥n luyá»‡n, kiá»ƒm Ä‘á»‹nh, vÃ  kiá»ƒm tra.

        Args:
            batch_size (int): KÃ­ch thÆ°á»›c batch.
            val_split (float): Tá»· lá»‡ dá»¯ liá»‡u kiá»ƒm Ä‘á»‹nh.
            test_split (float): Tá»· lá»‡ dá»¯ liá»‡u kiá»ƒm tra.
            shuffle (bool): CÃ³ xÃ¡o trá»™n dá»¯ liá»‡u hay khÃ´ng.

        Returns:
            Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]: Dataset huáº¥n luyá»‡n, kiá»ƒm Ä‘á»‹nh, vÃ  kiá»ƒm tra.
        """
        print("ğŸ”„ Loading data...")
        X_raw, y_raw = [], []
        data_generator = self.data_loader.load_data_generator(batch_size=1000, shuffle=True)
        for X_batch, y_batch in data_generator:
            X_raw.extend(X_batch.tolist())
            y_raw.extend(y_batch.tolist())
        print(f"âœ… Loaded {len(X_raw)} data samples")

        # ThÃªm giÃ¡ trá»‹ value giáº£ (0.5) vÃ¬ dá»¯ liá»‡u hiá»‡n táº¡i khÃ´ng cÃ³ nhÃ£n value
        # Trong thá»±c táº¿, cáº§n táº¡o nhÃ£n value (vÃ­ dá»¥: 1 náº¿u tháº¯ng, 0 náº¿u hÃ²a, -1 náº¿u thua)
        y_raw_with_value = [(y[0], y[1], y[2], 0.5) for y in y_raw]

        X_train_val, X_test, y_train_val, y_test = train_test_split(
            X_raw, y_raw_with_value, test_size=test_split, random_state=42
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_train_val, y_train_val, test_size=val_split/(1-test_split), random_state=42
        )
        print(f"ğŸ“Š Data split: Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")

        print("ğŸ”„ Preprocessing and creating datasets...")
        train_ds = self.preprocessor.create_tensorflow_dataset(X_train, y_train, batch_size, shuffle)
        val_ds = self.preprocessor.create_tensorflow_dataset(X_val, y_val, batch_size, shuffle=False)
        test_ds = self.preprocessor.create_tensorflow_dataset(X_test, y_test, batch_size, shuffle=False)
        print("âœ… Datasets created")
        return train_ds, val_ds, test_ds

    def train_model(self, model: Optional[ChessModel] = None, epochs: int = 100, batch_size: int = 32) -> ChessModel:
        """
        Huáº¥n luyá»‡n mÃ´ hÃ¬nh AI cá» vua.

        Args:
            model (Optional[ChessModel]): MÃ´ hÃ¬nh cáº§n huáº¥n luyá»‡n (táº¡o má»›i náº¿u None).
            epochs (int): Sá»‘ epoch huáº¥n luyá»‡n.
            batch_size (int): KÃ­ch thÆ°á»›c batch.

        Returns:
            ChessModel: MÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n.
        """
        train_ds, val_ds, _ = self.prepare_datasets(batch_size=batch_size)
        os.makedirs(self.model_save_dir, exist_ok=True)
        
        print("ğŸ”„ Initializing new model...")
        model = ChessModel()
        print("âœ… Model initialized")

        callbacks = [
            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),
            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(self.model_save_dir, 'chess_model_best.h5'),
                monitor='val_loss', save_best_only=True, verbose=1
            )
        ]

        print(f"ğŸ‹ï¸ Starting training with {epochs} epochs...")
        history = model.train(train_ds, val_ds, epochs, callbacks)
        print("âœ… Training completed")

        final_model_path = os.path.join(self.model_save_dir, 'chess_model_final.h5')
        model.save(final_model_path)
        print(f"ğŸ’¾ Saved final model at: {final_model_path}")

        self._plot_training_history(history)
        return model
       
    def _plot_training_history(self, history: Dict) -> None:
        """
        Váº½ vÃ  lÆ°u biá»ƒu Ä‘á»“ lá»‹ch sá»­ huáº¥n luyá»‡n.

        Args:
            history (Dict): Dictionary chá»©a lá»‹ch sá»­ huáº¥n luyá»‡n.
        """
        plt.figure(figsize=(15, 10))
        plt.subplot(2, 2, 1)
        plt.plot(history['loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Val Loss')
        plt.title('Loss During Training')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()

        plt.subplot(2, 2, 2)
        plt.plot(history['from_square_accuracy'], label='Train From Acc')
        plt.plot(history['val_from_square_accuracy'], label='Val From Acc')
        plt.title('From Square Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.subplot(2, 2, 3)
        plt.plot(history['to_square_accuracy'], label='Train To Acc')
        plt.plot(history['val_to_square_accuracy'], label='Val To Acc')
        plt.title('To Square Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()

        plt.subplot(2, 2, 4)
        plt.plot(history['value_mae'], label='Train Value MAE')
        plt.plot(history['val_value_mae'], label='Val Value MAE')
        plt.title('Value MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()

        plt.tight_layout()
        plt.savefig(os.path.join(self.model_save_dir, 'training_history.png'))
        print("ğŸ“Š Saved training history plot")

    def evaluate_model(self, model: Optional[ChessModel] = None) -> Dict:
        """
        ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra.

        Args:
            model (Optional[ChessModel]): MÃ´ hÃ¬nh cáº§n Ä‘Ã¡nh giÃ¡ (táº£i tá»« file náº¿u None).

        Returns:
            Dict: Dictionary chá»©a cÃ¡c metric Ä‘Ã¡nh giÃ¡.
        """
        if model is None:
            best_model_path = os.path.join(self.model_save_dir, 'chess_model_best.h5')
            model = ChessModel.load(best_model_path)
            print(f"âœ… Loaded model from: {best_model_path}")

        _, _, test_ds = self.prepare_datasets()
        print("ğŸ” Evaluating model...")
        metrics = model.model.evaluate(test_ds, verbose=1)
        result = {name: value for name, value in zip(model.model.metrics_names, metrics)}
        for name, value in result.items():
            print(f"ğŸ“ˆ {name}: {value:.4f}")
        return result